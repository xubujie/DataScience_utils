{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "data_path = '../../../../NLP/data/' # please change to your own path\n",
    "seq_len = 30\n",
    "fix_embedding = True # fix embedding during training\n",
    "batch_size = 128\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "emb_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_training_data(path='training_label.txt'):\n",
    "    if 'training_label' in path:\n",
    "        with open(data_path+path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip('\\n').split('+++$+++') for line in lines]\n",
    "        x = [line[1] for line in lines]\n",
    "        y = [int(line[0].strip()) for line in lines]\n",
    "        return x, y\n",
    "    else:\n",
    "        with open(data_path+path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            x = [line.strip('\\n') for line in lines]\n",
    "        return x\n",
    "def load_testing_data(path='testing_data.txt'):\n",
    "    # 把testing時需要的data讀進來\n",
    "    with open(data_path + path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec\n",
    "from gensim.models import word2vec\n",
    "def train_word2vec(x):\n",
    "    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=4, iter=10, sg=1)\n",
    "    return model\n",
    "\n",
    "# model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_x, train_y = load_training_data('training_label.txt')\n",
    "train_x_no_label = load_training_data('training_nolabel.txt')\n",
    "test_x = load_testing_data('testing_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace special character\n",
    "def preprocess(data):\n",
    "    '''\n",
    "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "    '''\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = [clean_special_chars(s, punct) for s in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_x + train_x_no_label + test_x)\n",
    "# clean punct\n",
    "train_x = preprocess(train_x)\n",
    "test_x = preprocess(test_x)\n",
    "# tokenize sentence and give each word index\n",
    "train_x = tokenizer.texts_to_sequences(train_x)\n",
    "test_x = tokenizer.texts_to_sequences(test_x)\n",
    "# cut sentence by maxlen\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=seq_len)\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build word embedding matrix\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "def build_embedding_matrix(word_index, embedding):\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, emb_size))\n",
    "    unknow_words = []\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding[word]\n",
    "        except KeyError:\n",
    "            unknow_words.append(word)\n",
    "    return embedding_matrix, unknow_words\n",
    "embedding = Word2Vec.load(data_path + 'word2vec.model')\n",
    "embedding_matrix, unknow_words = build_embedding_matrix(tokenizer.word_index, embedding.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Data loader\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class TwitterDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Expected data shape like:(data_num, data_len)\n",
    "    Data can be a list of numpy array or a list of lists\n",
    "    input data shape : (data_num, seq_len, feature_dim)\n",
    "    \n",
    "    __len__ will return the number of data\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: return self.data[idx]\n",
    "        return self.data[idx], self.label[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "class TwitterSentimentModel(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix, fix_embedding, hidden_dim, num_layers, dropout):\n",
    "        super(TwitterSentimentModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1])\n",
    "        self.embedding.weight = torch.nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        self.embedding_dim = embedding_matrix.shape[1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = torch.nn.LSTM(self.embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.classifier = torch.nn.Sequential( \n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        inputs = self.embedding(x)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        # x.shape = (batch_size, seq_len, hidden_size)\n",
    "        # get the last output\n",
    "        x = x[:, -1, :]\n",
    "        x = self.classifier(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Parameters:\n",
    "- **input_size** – The number of expected features in the input x\n",
    "- **hidden_size** – The number of features in the hidden state h\n",
    "- **num_layers** – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "- **bias** – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "- **batch_first** – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "- **dropout** – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "- **bidirectional** – If True, becomes a bidirectional LSTM. Default: False\n",
    "\n",
    "Input of LSTM: **input, (h_0, c_0)** `self.lstm(inputs, None)`\n",
    "- **input of shape (seq_len, batch, input_size)**: tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.\n",
    "- **h_0 of shape (num_layers * num_directions, batch, hidden_size)**: tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1.\n",
    "- **c_0 of shape (num_layers * num_directions, batch, hidden_size)**: tensor containing the initial cell state for each element in the batch.If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.\n",
    "\n",
    "Output of LSTM: **output, (h_n, c_n)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='LSTM1.png' width=\"500\" height=\"800\"></img>\n",
    "<img src='LSTM2.png' width=\"500\" height=\"800\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid loop\n",
    "def valid(model, val_loader, criterion):\n",
    "    total_loss, total_acc = 0., 0.\n",
    "    v_batch = len(val_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(val_loader):\n",
    "            x = x.to(device, dtype=torch.long)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "            outputs = model(x)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, y)\n",
    "            correct = accuracy(outputs, y, 0.5)\n",
    "            total_acc += (correct/batch_size)\n",
    "            total_loss += loss.item()\n",
    "        print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "    return total_loss/v_batch, total_acc/v_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "def save_checkpoint(state, is_best, checkpoint):\n",
    "    \"\"\"Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves\n",
    "    checkpoint + 'best.pth.tar'\n",
    "\n",
    "    Args:\n",
    "        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n",
    "        is_best: (bool) True if it is the best model seen till now\n",
    "        checkpoint: (string) folder where parameters are to be saved\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(checkpoint, 'last.pth.tar')\n",
    "    if not os.path.exists(checkpoint):\n",
    "        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n",
    "        os.mkdir(checkpoint)\n",
    "#     else:\n",
    "#         print(\"Checkpoint Directory exists! \")\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'best.pth.tar'))\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer=None):\n",
    "    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
    "    optimizer assuming it is present in checkpoint.\n",
    "\n",
    "    Args:\n",
    "        checkpoint: (string) filename which needs to be loaded\n",
    "        model: (torch.nn.Module) model for which the parameters are loaded\n",
    "        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise (\"File doesn't exist {}\".format(checkpoint))\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optim_dict'])\n",
    "\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, n_epochs, model_dir, restore_file=None):\n",
    "    if restore_file is not None:\n",
    "        load_checkpoint(restore_file, model, optimizer) \n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
    "    model.train()\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "    t_batch = len(train_loader)\n",
    "    v_batch = len(val_loader)\n",
    "    show_idx = 50\n",
    "    writer = SummaryWriter()\n",
    "    with torch.enable_grad():\n",
    "        for epoch in range(n_epochs):\n",
    "            total_loss, total_acc = 0., 0.\n",
    "            for i, (x, y) in enumerate(train_loader):\n",
    "                x = x.to(device, dtype=torch.long)\n",
    "                y = y.to(device, dtype=torch.float)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                correct = accuracy(outputs, y, 0.5)\n",
    "                total_acc += (correct/batch_size)\n",
    "                total_loss += loss.item()\n",
    "                if (i+1) % show_idx == 0:\n",
    "                    print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
    "                        epoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
    "            print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
    "            val_loss, val_acc = valid(model, val_loader, criterion)\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                print('---find best score---')\n",
    "                save_checkpoint({'epoch': epoch + 1,\n",
    "                                'state_dict': model.state_dict(),\n",
    "                                'optim_dict': optimizer.state_dict()},\n",
    "                                is_best=True,\n",
    "                                checkpoint=model_dir)\n",
    "            print('-----------------------------------------------')\n",
    "            model.train() # 將model的模式設為train，這樣optimizer就可以更新model的參數（因為剛剛轉成eval模式）\n",
    "            writer.add_scalar('Loss/train', total_loss/t_batch, epoch)\n",
    "            writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "            writer.add_scalar('Accuracy/train', total_acc/t_batch*100, epoch)\n",
    "            writer.add_scalar('Accuracy/val', val_acc, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "def accuracy(outputs, labels, threshold):\n",
    "    outputs[outputs>=threshold] = 1\n",
    "    outputs[outputs<threshold] = 0\n",
    "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_train = int(len(train_x)*0.8)\n",
    "tr_x, val_x = train_x[:num_train], train_x[num_train:]\n",
    "tr_y, val_y = train_y[:num_train], train_y[num_train:]\n",
    "# train, val data loader\n",
    "train_dataset = TwitterDataset(X=tr_x, y=tr_y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                    batch_size = batch_size,\n",
    "                                    shuffle = True)\n",
    "val_dataset = TwitterDataset(X=val_x, y=val_y)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                    batch_size = batch_size,\n",
    "                                    shuffle = False)\n",
    "\n",
    "\n",
    "model = TwitterSentimentModel(embedding_matrix, fix_embedding=True, hidden_dim=256, num_layers=1, dropout=0.2)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training, parameter total:64063199, trainable:520449\n",
      "\n",
      "[ Epoch1: 1250/1250 ] loss:0.491 acc:75.000 \n",
      "Train | Loss:0.46112 Acc: 78.278\n",
      "Valid | Loss:0.43288 Acc: 79.673 \n",
      "-----------------------------------------------\n",
      "[ Epoch2: 1250/1250 ] loss:0.379 acc:83.594 \n",
      "Train | Loss:0.41651 Acc: 80.839\n",
      "Valid | Loss:0.42033 Acc: 80.696 \n",
      "-----------------------------------------------\n",
      "[ Epoch3: 1250/1250 ] loss:0.388 acc:80.469 \n",
      "Train | Loss:0.39562 Acc: 82.019\n",
      "Valid | Loss:0.41438 Acc: 80.791 \n",
      "-----------------------------------------------\n",
      "[ Epoch4: 1250/1250 ] loss:0.387 acc:81.250 \n",
      "Train | Loss:0.37823 Acc: 82.895\n",
      "Valid | Loss:0.40145 Acc: 81.842 \n",
      "-----------------------------------------------\n",
      "[ Epoch5: 1250/1250 ] loss:0.378 acc:82.812 \n",
      "Train | Loss:0.36040 Acc: 83.834\n",
      "Valid | Loss:0.40268 Acc: 81.849 \n",
      "-----------------------------------------------\n",
      "[ Epoch6: 1250/1250 ] loss:0.351 acc:85.938 \n",
      "Train | Loss:0.33891 Acc: 84.904\n",
      "Valid | Loss:0.41217 Acc: 81.577 \n",
      "-----------------------------------------------\n",
      "[ Epoch7: 1250/1250 ] loss:0.338 acc:87.500 \n",
      "Train | Loss:0.31180 Acc: 86.243\n",
      "Valid | Loss:0.43767 Acc: 81.138 \n",
      "-----------------------------------------------\n",
      "[ Epoch8: 1250/1250 ] loss:0.289 acc:85.938 \n",
      "Train | Loss:0.28002 Acc: 87.806\n",
      "Valid | Loss:0.45205 Acc: 80.771 \n",
      "-----------------------------------------------\n",
      "[ Epoch9: 1250/1250 ] loss:0.189 acc:92.188 \n",
      "Train | Loss:0.24310 Acc: 89.626\n",
      "Valid | Loss:0.49540 Acc: 80.464 \n",
      "-----------------------------------------------\n",
      "[ Epoch10: 1250/1250 ] loss:0.171 acc:95.312 \n",
      "Train | Loss:0.20474 Acc: 91.378\n",
      "Valid | Loss:0.53693 Acc: 79.865 \n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, criterion, optimizer, 10, 'model_dir', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict no label data\n",
    "train_x_no_label = load_training_data('training_nolabel.txt')\n",
    "train_x_no_label = preprocess(train_x_no_label)\n",
    "train_x_no_label = tokenizer.texts_to_sequences(train_x_no_label)\n",
    "train_x_no_label = sequence.pad_sequences(train_x_no_label, maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the no label data to train\n",
    "train_x_no_label_dataset = TwitterDataset(train_x_no_label, None)\n",
    "train_x_no_label_dataloader = torch.utils.data.DataLoader(dataset = train_x_no_label_dataset,\n",
    "                                    batch_size = batch_size,\n",
    "                                    shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = TwitterSentimentModel(embedding_matrix, fix_embedding=True, hidden_dim=256, num_layers=1, dropout=0.2)\n",
    "load_checkpoint('model_dir/best.pth.tar', model, optimizer)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "pred = []\n",
    "with torch.no_grad():\n",
    "    for i, x in enumerate(train_x_no_label_dataloader):\n",
    "        x = x.to(device, dtype=torch.long)\n",
    "        outputs = model(x)\n",
    "        outputs = outputs.squeeze()\n",
    "        pred += outputs.float().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(pred)\n",
    "\n",
    "train_x_semi = train_x_no_label[(pred>=0.8) | (pred <=0.2)]\n",
    "y_semi = (pred[(pred>=0.8) | (pred <=0.2)]>=0.5).astype('int')\n",
    "\n",
    "new_train_x = np.concatenate([tr_x, train_x_semi])\n",
    "new_y = np.concatenate([tr_y, y_semi])\n",
    "\n",
    "new_train_dataset = TwitterDataset(new_train_x, new_y)\n",
    "new_train_dataloader = torch.utils.data.DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training, parameter total:64063199, trainable:520449\n",
      "\n",
      "[ Epoch1: 7100/7129 ] loss:0.134 acc:95.312  \n",
      "Train | Loss:0.12113 Acc: 97.428\n",
      "Valid | Loss:0.40268 Acc: 81.849 \n",
      "---find best score---\n",
      "-----------------------------------------------\n",
      "[ Epoch2: 7100/7129 ] loss:0.102 acc:99.219  \n",
      "Train | Loss:0.12110 Acc: 97.427\n",
      "Valid | Loss:0.40268 Acc: 81.849 \n",
      "-----------------------------------------------\n",
      "[ Epoch3: 7100/7129 ] loss:0.128 acc:96.875  \n",
      "Train | Loss:0.12110 Acc: 97.414\n",
      "Valid | Loss:0.40268 Acc: 81.849 \n",
      "-----------------------------------------------\n",
      "[ Epoch4: 7100/7129 ] loss:0.128 acc:97.656  \n",
      "Train | Loss:0.12113 Acc: 97.423\n",
      "Valid | Loss:0.40268 Acc: 81.849 \n",
      "-----------------------------------------------\n",
      "[ Epoch5: 7100/7129 ] loss:0.127 acc:96.094  \n",
      "Train | Loss:0.12108 Acc: 97.421\n",
      "Valid | Loss:0.40268 Acc: 81.849 \n",
      "-----------------------------------------------\n",
      "[ Epoch6: 7100/7129 ] loss:0.109 acc:98.438  \n",
      "Train | Loss:0.12118 Acc: 97.421\n",
      "Valid | Loss:0.40268 Acc: 81.849 \n",
      "-----------------------------------------------\n",
      "[ Epoch7: 7100/7129 ] loss:0.122 acc:96.875  \n",
      "Train | Loss:0.12115 Acc: 97.429\n",
      "Valid | Loss:0.40268 Acc: 81.849 \n",
      "-----------------------------------------------\n",
      "[ Epoch8: 7100/7129 ] loss:0.141 acc:96.094  \n",
      "Train | Loss:0.12113 Acc: 97.422\n",
      "Valid | Loss:0.40268 Acc: 81.849 \n",
      "-----------------------------------------------\n",
      "[ Epoch9: 7100/7129 ] loss:0.102 acc:98.438  \n",
      "Train | Loss:0.12109 Acc: 97.428\n",
      "Valid | Loss:0.40268 Acc: 81.849 \n",
      "-----------------------------------------------\n",
      "[ Epoch10: 7100/7129 ] loss:0.146 acc:94.531  \n",
      "Train | Loss:0.12108 Acc: 97.423\n",
      "Valid | Loss:0.40268 Acc: 81.849 \n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train(model, new_train_dataloader, val_loader, criterion, optimizer, 10, 'semi', 'model_dir/best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
